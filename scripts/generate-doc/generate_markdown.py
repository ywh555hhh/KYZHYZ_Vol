#!/usr/bin/env python3
"""
Convert chapter JSON files to Markdown learning documents.
Each Markdown file contains rich educational content for 90 words.
"""

import json
import os
import math
import re
from typing import List, Dict, Any

class VocabularyMarkdownGenerator:
    """Generate rich Markdown content for vocabulary learning."""
    
    def __init__(self):
        self.phonetic_data = self._load_phonetic_data()
    
    def _load_phonetic_data(self) -> Dict[str, str]:
        """Load basic phonetic data for common words."""
        # In a real implementation, this would load from a phonetic dictionary
        # For now, we'll create basic phonetic patterns
        common_phonetics = {
            "the": "/Ã°É™/", "be": "/biË/", "a": "/É™/", "to": "/tuË/", "of": "/ÊŒv/",
            "and": "/Ã¦nd/", "in": "/Éªn/", "have": "/hÃ¦v/", "it": "/Éªt/", "for": "/fÉ”Ër/",
            "not": "/nÉ’t/", "with": "/wÉªÃ°/", "as": "/Ã¦z/", "you": "/juË/", "do": "/duË/",
            "at": "/Ã¦t/", "this": "/Ã°Éªs/", "but": "/bÊŒt/", "his": "/hÉªz/", "by": "/baÉª/",
            "from": "/frÊŒm/", "they": "/Ã°eÉª/", "she": "/ÊƒiË/", "or": "/É”Ër/", "an": "/Ã¦n/",
            "will": "/wÉªl/", "my": "/maÉª/", "one": "/wÊŒn/", "all": "/É”Ël/", "would": "/wÊŠd/",
            "there": "/Ã°eÉ™r/", "their": "/Ã°eÉ™r/", "what": "/wÊŒt/", "so": "/sÉ™ÊŠ/", "up": "/ÊŒp/",
            "out": "/aÊŠt/", "if": "/Éªf/", "about": "/É™ËˆbaÊŠt/", "who": "/huË/", "get": "/get/",
            "which": "/wÉªtÊƒ/", "go": "/gÉ™ÊŠ/", "me": "/miË/", "when": "/wen/", "make": "/meÉªk/",
            "can": "/kÃ¦n/", "like": "/laÉªk/", "time": "/taÉªm/", "no": "/nÉ™ÊŠ/", "just": "/dÊ’ÊŒst/",
            "him": "/hÉªm/", "know": "/nÉ™ÊŠ/", "take": "/teÉªk/", "people": "/ËˆpiËpl/", "into": "/ËˆÉªntuË/",
            "year": "/jÉªÉ™r/", "your": "/jÊŠÉ™r/", "good": "/gÊŠd/", "some": "/sÊŒm/", "could": "/kÊŠd/",
            "them": "/Ã°em/", "see": "/siË/", "other": "/ËˆÊŒÃ°É™r/", "than": "/Ã°Ã¦n/", "then": "/Ã°en/",
            "now": "/naÊŠ/", "look": "/lÊŠk/", "only": "/ËˆÉ™ÊŠnli/", "come": "/kÊŒm/", "its": "/Éªts/",
            "over": "/ËˆÉ™ÊŠvÉ™r/", "think": "/Î¸ÉªÅ‹k/", "also": "/ËˆÉ”ËlsÉ™ÊŠ/", "your": "/jÊŠÉ™r/", "work": "/wÉœËrk/",
            "life": "/laÉªf/", "way": "/weÉª/", "may": "/meÉª/", "say": "/seÉª/", "each": "/iËtÊƒ/",
            "which": "/wÉªtÊƒ/", "she": "/ÊƒiË/", "do": "/duË/", "how": "/haÊŠ/", "their": "/Ã°eÉ™r/",
            "if": "/Éªf/", "will": "/wÉªl/", "up": "/ÊŒp/", "other": "/ËˆÊŒÃ°É™r/", "about": "/É™ËˆbaÊŠt/",
            "out": "/aÊŠt/", "many": "/Ëˆmeni/", "then": "/Ã°en/", "them": "/Ã°em/", "these": "/Ã°iËz/",
            "so": "/sÉ™ÊŠ/", "some": "/sÊŒm/", "her": "/hÉ™r/", "would": "/wÊŠd/", "make": "/meÉªk/",
            "like": "/laÉªk/", "into": "/ËˆÉªntuË/", "him": "/hÉªm/", "has": "/hÃ¦z/", "two": "/tuË/",
            "more": "/mÉ”Ër/", "very": "/Ëˆveri/", "what": "/wÊŒt/", "know": "/nÉ™ÊŠ/", "just": "/dÊ’ÊŒst/",
            "first": "/fÉœËrst/", "get": "/get/", "over": "/ËˆÉ™ÊŠvÉ™r/", "think": "/Î¸ÉªÅ‹k/", "where": "/weÉ™r/",
            "much": "/mÊŒtÊƒ/", "go": "/gÉ™ÊŠ/", "well": "/wel/", "were": "/wÉ™r/", "been": "/biËn/",
            "through": "/Î¸ruË/", "when": "/wen/", "who": "/huË/", "oil": "/É”Éªl/", "its": "/Éªts/",
            "now": "/naÊŠ/", "find": "/faÉªnd/", "long": "/lÉ”ËÅ‹/", "down": "/daÊŠn/", "day": "/deÉª/",
            "did": "/dÉªd/", "get": "/get/", "come": "/kÊŒm/", "made": "/meÉªd/", "may": "/meÉª/",
            "part": "/pÉ‘Ërt/"
        }
        return common_phonetics
    
    def _get_phonetic(self, word: str) -> str:
        """Get phonetic transcription for a word."""
        word_lower = word.lower()
        if word_lower in self.phonetic_data:
            return self.phonetic_data[word_lower]
        else:
            # Generate a placeholder phonetic based on word patterns
            return self._generate_phonetic_placeholder(word_lower)
    
    def _generate_phonetic_placeholder(self, word: str) -> str:
        """Generate a basic phonetic placeholder for unknown words."""
        # This is a simplified version - in reality you'd use a phonetic library
        phonetic = f"/{word}/"  # Placeholder
        return phonetic
    
    def _get_word_derivatives(self, word: str, pos: str = None) -> List[str]:
        """Get common derivatives of a word."""
        derivatives = []
        word_lower = word.lower()
        
        # Common patterns for derivatives
        if word_lower.endswith('e'):
            base = word_lower[:-1]
            derivatives.extend([
                f"{base}ing *v.* (ç°åœ¨åˆ†è¯)",
                f"{base}ed *v.* (è¿‡å»å¼/è¿‡å»åˆ†è¯)"
            ])
        elif word_lower.endswith('y'):
            base = word_lower[:-1]
            derivatives.extend([
                f"{base}ies *n.* (å¤æ•°)",
                f"{base}ied *v.* (è¿‡å»å¼)"
            ])
        
        # Add -ly for adjectives
        if not word_lower.endswith('ly'):
            derivatives.append(f"{word_lower}ly *adv.* (å‰¯è¯)")
        
        # Add -ness for adjectives
        if not word_lower.endswith('ness'):
            derivatives.append(f"{word_lower}ness *n.* (åè¯)")
        
        return derivatives[:3]  # Limit to 3 derivatives
    
    def _generate_example_sentences(self, word: str, definition: str) -> List[Dict[str, str]]:
        """Generate example sentences for a word."""
        word_lower = word.lower()
        
        # Common sentence patterns based on word type
        sentences = [
            {
                "english": f"The {word_lower} is very important in our daily life.",
                "chinese": f"è¿™ä¸ª{definition}åœ¨æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ä¸­éå¸¸é‡è¦ã€‚"
            },
            {
                "english": f"We need to understand how to use {word_lower} correctly.",
                "chinese": f"æˆ‘ä»¬éœ€è¦ç†è§£å¦‚ä½•æ­£ç¡®ä½¿ç”¨{definition}ã€‚"
            }
        ]
        
        return sentences
    
    def _generate_cultural_note(self, word: str) -> str:
        """Generate cultural note for a word."""
        cultural_notes = {
            "the": "å®šå† è¯'the'æ˜¯è‹±è¯­ä¸­æœ€å¸¸ç”¨çš„è¯ï¼Œç”¨äºç‰¹æŒ‡å·²çŸ¥çš„äººæˆ–ç‰©ã€‚åœ¨è€ƒç ”è‹±è¯­ä¸­ï¼ŒæŒæ¡å† è¯çš„ç”¨æ³•æ˜¯åŸºç¡€ä¸­çš„åŸºç¡€ã€‚",
            "be": "åŠ¨è¯'be'æ˜¯è‹±è¯­ä¸­æœ€é‡è¦çš„ç³»åŠ¨è¯ï¼Œæœ‰am/is/areç­‰å½¢å¼ã€‚å®ƒæ˜¯æ„æˆå„ç§æ—¶æ€å’Œè¯­æ€çš„åŸºç¡€ã€‚",
            "and": "è¿è¯'and'è¡¨ç¤ºå¹¶åˆ—å…³ç³»ï¼Œæ˜¯è‹±è¯­å†™ä½œä¸­æœ€åŸºæœ¬çš„è¿æ¥è¯ä¹‹ä¸€ã€‚",
            "in": "ä»‹è¯'in'è¡¨ç¤ºæ—¶é—´ã€åœ°ç‚¹ã€æ–¹å¼ç­‰ï¼Œæ˜¯è€ƒç ”è‹±è¯­ä¸­å¸¸è€ƒçš„ä»‹è¯ä¹‹ä¸€ã€‚"
        }
        
        return cultural_notes.get(word.lower(), f"'{word}'æ˜¯è€ƒç ”è¯æ±‡ä¸­çš„é‡è¦å•è¯ï¼Œéœ€è¦æŒæ¡å…¶å„ç§ç”¨æ³•å’Œæ­é…ã€‚")
    
    def _generate_multiple_meanings(self, word: str, definition: str) -> List[str]:
        """Generate multiple meanings for a word."""
        meanings = [
            f"åŸºæœ¬å«ä¹‰ï¼š{definition}",
            f"åœ¨ä¸åŒè¯­å¢ƒä¸­å¯è¡¨ç¤ºï¼š{definition}çš„ç›¸å…³æ¦‚å¿µ",
            f"è€ƒç ”å¸¸è€ƒå«ä¹‰ï¼š{definition}åŠå…¶å¼•ç”³ä¹‰"
        ]
        return meanings
    
    def _generate_word_content(self, word_data: Dict[str, Any], index: int) -> str:
        """Generate detailed content for a single word."""
        word = word_data["å•è¯"]
        definition = word_data["é‡Šä¹‰"]
        frequency = word_data["è¯é¢‘"]
        sequence = word_data["åºå·"]
        variant = word_data.get("å…¶ä»–æ‹¼å†™")
        
        phonetic = self._get_phonetic(word)
        derivatives = self._get_word_derivatives(word)
        examples = self._generate_example_sentences(word, definition)
        cultural_note = self._generate_cultural_note(word)
        multiple_meanings = self._generate_multiple_meanings(word, definition)
        
        # Generate emoji based on index
        emoji_list = ["1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£", "5ï¸âƒ£", "6ï¸âƒ£", "7ï¸âƒ£", "8ï¸âƒ£", "9ï¸âƒ£", "ğŸ”Ÿ", 
                     "ğŸŒŸ", "ğŸ’«", "â­", "âœ¨", "ğŸ¯", "ğŸ†", "ğŸ“š", "ğŸ’", "ğŸ”¥", "âš¡",
                     "ğŸª", "ğŸ­", "ğŸ¨", "ğŸµ", "ğŸ¸", "ğŸº", "ğŸ»", "ğŸ¹", "ğŸ¥", "ğŸ¤"]
        emoji = emoji_list[index % len(emoji_list)]
        
        content = f"""### {emoji} {word} `{phonetic}`

**ã€åŸºæœ¬é‡Šä¹‰ã€‘** {definition}

**ã€è¯é¢‘æ’åºã€‘** ç¬¬{sequence}ä½ | è¯é¢‘: {frequency}æ¬¡"""
        
        if variant:
            content += f"\n**ã€å…¶ä»–æ‹¼å†™ã€‘** {variant}"
        
        if derivatives:
            content += f"\n\n**ã€è¯æ€§å˜åŒ–ã€‘**\n"
            for i, derivative in enumerate(derivatives[:3], 1):
                content += f"- {derivative}\n"
        
        content += f"""
**ã€é‡ç‚¹è¾¨æã€‘**
è€ƒç ”ä¸­éœ€è¦é‡ç‚¹å…³æ³¨"{word}"çš„ç”¨æ³•å’Œæ­é…ï¼Œç‰¹åˆ«æ˜¯åœ¨é˜…è¯»ç†è§£å’Œå®Œå½¢å¡«ç©ºä¸­çš„åº”ç”¨ã€‚

**ã€è€ƒç‚¹èšç„¦ã€‘**
1. é«˜é¢‘æ­é…ï¼šå¸¸ä¸å…¶ä»–è¯ç»„æˆå›ºå®šæ­é…
2. è¯­æ³•è¦ç‚¹ï¼šæ³¨æ„è¯æ€§å’Œç”¨æ³•
3. è€ƒè¯•é‡ç‚¹ï¼šåœ¨è€ƒç ”çœŸé¢˜ä¸­çš„å¸¸è§ç”¨æ³•

**ã€ä¾‹å¥ç²¾è®²ã€‘**"""
        
        for i, example in enumerate(examples, 1):
            content += f"""
> {example['english']}
> *{example['chinese']}*
"""
        
        content += f"""
**ã€æ–‡åŒ–é“¾æ¥ã€‘**
{cultural_note}

**ã€ä¸€è¯å¤šä¹‰ã€‘**"""
        
        for i, meaning in enumerate(multiple_meanings, 1):
            content += f"\n{i}. {meaning}"
        
        content += "\n\n---\n"
        
        return content
    
    def _generate_section_summary(self, section_num: int, words: List[Dict]) -> str:
        """Generate summary for a section."""
        word_list = [w["å•è¯"] for w in words]
        word_str = " â€¢ ".join(word_list)
        
        return f"""## ğŸ“‹ ç¬¬{section_num}èŠ‚ å­¦ä¹ æ€»ç»“

**æœ¬èŠ‚é‡ç‚¹å•è¯ï¼š** {word_str}

### ğŸ¯ å­¦ä¹ è¦ç‚¹
1. **é«˜é¢‘è¯æ±‡**ï¼šæœ¬èŠ‚åŒ…å«{len(words)}ä¸ªé‡è¦è¯æ±‡ï¼Œéƒ½æ˜¯è€ƒç ”è‹±è¯­ä¸­çš„åŸºç¡€è¯æ±‡
2. **è®°å¿†ç­–ç•¥**ï¼šå»ºè®®é‡‡ç”¨è¯æ ¹è¯ç¼€è®°å¿†æ³•ï¼Œç»“åˆä¾‹å¥åŠ æ·±ç†è§£
3. **åº”ç”¨é‡ç‚¹**ï¼šè¿™äº›è¯æ±‡åœ¨é˜…è¯»ç†è§£ã€å†™ä½œå’Œç¿»è¯‘ä¸­éƒ½æœ‰é‡è¦åº”ç”¨

### ğŸ“ å­¦ä¹ å»ºè®®
- æ¯å¤©å¤ä¹ æœ¬èŠ‚å•è¯ï¼Œç¡®ä¿ç†Ÿç»ƒæŒæ¡åŸºæœ¬å«ä¹‰
- é‡ç‚¹å…³æ³¨ä¸€è¯å¤šä¹‰å’Œå›ºå®šæ­é…
- ç»“åˆçœŸé¢˜ç»ƒä¹ ï¼Œæé«˜å®é™…åº”ç”¨èƒ½åŠ›

---

"""
    
    def generate_chapter_markdown(self, chapter_file: str, output_dir: str) -> str:
        """Generate a complete Markdown file for a chapter."""
        
        # Read chapter JSON
        with open(chapter_file, 'r', encoding='utf-8') as f:
            chapter_data = json.load(f)
        
        chapter_info = chapter_data["chapter_info"]
        words = chapter_data["words"]
        
        chapter_num = chapter_info["chapter_number"]
        word_count = chapter_info["word_count"]
        words_range = chapter_info["words_range"]
        
        # Create markdown content
        markdown_content = f"""# ğŸ“– è€ƒç ”è¯æ±‡å­¦ä¹ _ç¬¬{chapter_num}ç« 

> **è¯æ±‡èŒƒå›´ï¼š** ç¬¬{words_range}ä¸ªå•è¯ | **æ€»è¯æ•°ï¼š** {word_count}ä¸ª
> 
> **å­¦ä¹ ç›®æ ‡ï¼š** æŒæ¡æœ¬ç« æ‰€æœ‰è¯æ±‡çš„åŸºæœ¬å«ä¹‰ã€ç”¨æ³•å’Œæ­é…

---

## ğŸŒŸ ç« èŠ‚æ¦‚è§ˆ

æœ¬ç« åŒ…å«è€ƒç ”è¯æ±‡ä¸­çš„{word_count}ä¸ªé‡è¦å•è¯ï¼ŒæŒ‰ç…§è¯é¢‘æ’åºã€‚æ¯ä¸ªå•è¯éƒ½æä¾›äº†è¯¦ç»†çš„å­¦ä¹ å†…å®¹ï¼ŒåŒ…æ‹¬éŸ³æ ‡ã€ä¾‹å¥ã€æ–‡åŒ–èƒŒæ™¯ç­‰ã€‚

### ğŸ“Š æœ¬ç« å•è¯ä¸€è§ˆ

| åºå· | å•è¯ | éŸ³æ ‡ | åŸºæœ¬é‡Šä¹‰ | è¯é¢‘ |
|------|------|------|----------|------|"""
        
        # Add word overview table
        for word_data in words:
            word = word_data["å•è¯"]
            definition = word_data["é‡Šä¹‰"]
            sequence = word_data["åºå·"]
            frequency = word_data["è¯é¢‘"]
            phonetic = self._get_phonetic(word)
            markdown_content += f"\n| {sequence} | {word} | `{phonetic}` | {definition} | {frequency} |"
        
        markdown_content += "\n\n---\n\n"
        
        # Divide words into 3 sections (30 words each)
        section_size = 30
        sections = []
        for i in range(0, len(words), section_size):
            sections.append(words[i:i + section_size])
        
        # Generate content for each section
        for section_num, section_words in enumerate(sections, 1):
            markdown_content += f"## ğŸ“š ç¬¬{section_num}èŠ‚ (å•è¯ {section_words[0]['åºå·']}-{section_words[-1]['åºå·']})\n\n"
            
            # Add detailed content for each word in section
            for i, word_data in enumerate(section_words):
                word_content = self._generate_word_content(word_data, i)
                markdown_content += word_content
            
            # Add section summary
            section_summary = self._generate_section_summary(section_num, section_words)
            markdown_content += section_summary
        
        # Add chapter conclusion
        high_freq_words = [w["å•è¯"] for w in words if w["è¯é¢‘"] > 1000]
        markdown_content += f"""## ğŸ“ ç« èŠ‚æ€»ç»“

### âœ¨ æœ¬ç« äº®ç‚¹
1. **é«˜é¢‘æ ¸å¿ƒè¯**ï¼šæœ¬ç« åŒ…å«{len(high_freq_words)}ä¸ªé«˜é¢‘è¯æ±‡
2. **å­¦ä¹ ä»·å€¼**ï¼šè¿™äº›è¯æ±‡æ˜¯è€ƒç ”è‹±è¯­çš„åŸºç¡€ï¼Œå¿…é¡»ç†Ÿç»ƒæŒæ¡
3. **åº”ç”¨å¹¿æ³›**ï¼šåœ¨é˜…è¯»ã€å†™ä½œã€ç¿»è¯‘ä¸­éƒ½æœ‰é‡è¦ä½œç”¨

### ğŸ¯ é‡ç‚¹å•è¯å›é¡¾
{' â€¢ '.join(high_freq_words[:10])}{'...' if len(high_freq_words) > 10 else ''}

### ğŸ“ˆ å­¦ä¹ è¿›åº¦
- âœ… å·²å­¦ä¹ å•è¯ï¼š{word_count}ä¸ª
- ğŸ¯ å½“å‰è¿›åº¦ï¼šç¬¬{words_range}ä¸ªå•è¯
- ğŸ“Š å®Œæˆåº¦ï¼š{chapter_num}/{chapter_info['total_chapters']}ç« 

---

## ğŸ’¡ å­¦ä¹ å»ºè®®

### ğŸ”„ å¤ä¹ ç­–ç•¥
1. **æ—¥å¸¸å¤ä¹ **ï¼šæ¯å¤©èŠ±15-20åˆ†é’Ÿå¤ä¹ æœ¬ç« å•è¯
2. **è”æƒ³è®°å¿†**ï¼šåˆ©ç”¨è¯æ ¹è¯ç¼€å’Œè”æƒ³æ³•å¢å¼ºè®°å¿†
3. **å®é™…åº”ç”¨**ï¼šåœ¨é˜…è¯»å’Œå†™ä½œä¸­ä¸»åŠ¨ä½¿ç”¨è¿™äº›è¯æ±‡

### ğŸ“ ç»ƒä¹ å»ºè®®
1. **è¯æ±‡æµ‹è¯•**ï¼šå®šæœŸè¿›è¡Œè¯æ±‡æµ‹è¯•ï¼Œæ£€éªŒæŒæ¡ç¨‹åº¦
2. **é€ å¥ç»ƒä¹ **ï¼šç”¨æ¯ä¸ªå•è¯é€ å¥ï¼ŒåŠ æ·±ç†è§£
3. **çœŸé¢˜ç»ƒä¹ **ï¼šç»“åˆè€ƒç ”çœŸé¢˜ï¼Œæé«˜å®æˆ˜èƒ½åŠ›

### ğŸª è®°å¿†å°æŠ€å·§
- åˆ¶ä½œè¯æ±‡å¡ç‰‡ï¼Œéšæ—¶å¤ä¹ 
- å°†ç”Ÿè¯èå…¥æ—¥å¸¸å¯¹è¯å’Œå†™ä½œ
- åˆ©ç”¨è¯æ±‡Appè¿›è¡Œç¢ç‰‡åŒ–å­¦ä¹ 

---

*ğŸ“š **æŒç»­å­¦ä¹ ï¼Œç¨³æ­¥æå‡ï¼æ¯ä¸€ä¸ªå•è¯éƒ½æ˜¯é€šå‘æˆåŠŸçš„é˜¶æ¢¯ï¼** ğŸŒˆ*

> **ä¸‹ä¸€æ­¥ï¼š** ç»§ç»­å­¦ä¹ ç¬¬{chapter_num + 1}ç« ï¼Œä¿æŒå­¦ä¹ çš„è¿ç»­æ€§å’Œç³»ç»Ÿæ€§ã€‚
"""
        
        # Save to output directory
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"è€ƒç ”è¯æ±‡å­¦ä¹ _ç¬¬{chapter_num}ç« .md")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return output_file

def main():
    """Main function to generate all Markdown files."""
    generator = VocabularyMarkdownGenerator()
    
    # Create main output directory
    base_output_dir = "vocabulary_markdown"
    os.makedirs(base_output_dir, exist_ok=True)
    
    chapter_json_dir = "chapter_jsons"
    
    # Get all chapter JSON files
    chapter_files = []
    for filename in os.listdir(chapter_json_dir):
        if filename.startswith("chapter_") and filename.endswith(".json"):
            chapter_files.append(os.path.join(chapter_json_dir, filename))
    
    chapter_files.sort()
    
    print(f"Found {len(chapter_files)} chapter files to convert")
    
    # Process chapters and organize into folders (5 chapters per folder)
    chapters_per_folder = 5
    created_files = []
    
    for i, chapter_file in enumerate(chapter_files):
        chapter_num = i + 1
        folder_start = ((chapter_num - 1) // chapters_per_folder) * chapters_per_folder + 1
        folder_end = min(folder_start + chapters_per_folder - 1, len(chapter_files))
        
        # Create folder for every 5 chapters
        folder_name = f"è€ƒç ”è¯æ±‡_ç¬¬{folder_start}-{folder_end}ç« "
        folder_path = os.path.join(base_output_dir, folder_name)
        
        # Generate Markdown file
        output_file = generator.generate_chapter_markdown(chapter_file, folder_path)
        created_files.append(output_file)
        
        print(f"Created: {output_file}")
    
    print(f"\nâœ… Successfully created {len(created_files)} Markdown files!")
    print(f"ğŸ“ Files are organized in the '{base_output_dir}' directory")
    
    # Create summary report
    import time
    current_time = time.strftime('%Y-%m-%d %H:%M:%S')
    summary_file = os.path.join(base_output_dir, "ç”ŸæˆæŠ¥å‘Š.md")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"""# ğŸ“Š è€ƒç ”è¯æ±‡Markdownæ–‡æ¡£ç”ŸæˆæŠ¥å‘Š

## ğŸ¯ ç”Ÿæˆæ¦‚å†µ
- **æ€»æ–‡ä»¶æ•°ï¼š** {len(created_files)}ä¸ª
- **æ€»è¯æ±‡æ•°ï¼š** 5530ä¸ª
- **æ–‡ä»¶ç»„ç»‡ï¼š** æ¯{chapters_per_folder}ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡ä»¶å¤¹
- **ç”Ÿæˆæ—¶é—´ï¼š** {current_time}

## ğŸ“ æ–‡ä»¶ç»“æ„
```
vocabulary_markdown/
â”œâ”€â”€ è€ƒç ”è¯æ±‡_ç¬¬1-5ç« /
â”‚   â”œâ”€â”€ è€ƒç ”è¯æ±‡å­¦ä¹ _ç¬¬1ç« .md (1-90è¯)
â”‚   â”œâ”€â”€ è€ƒç ”è¯æ±‡å­¦ä¹ _ç¬¬2ç« .md (91-180è¯)
â”‚   â”œâ”€â”€ è€ƒç ”è¯æ±‡å­¦ä¹ _ç¬¬3ç« .md (181-270è¯)
â”‚   â”œâ”€â”€ è€ƒç ”è¯æ±‡å­¦ä¹ _ç¬¬4ç« .md (271-360è¯)
â”‚   â””â”€â”€ è€ƒç ”è¯æ±‡å­¦ä¹ _ç¬¬5ç« .md (361-450è¯)
â”œâ”€â”€ è€ƒç ”è¯æ±‡_ç¬¬6-10ç« /
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## âœ¨ æ–‡ä»¶ç‰¹è‰²
- ğŸ¨ ä¸°å¯Œçš„emojiè£…é¥°
- ğŸ“Š è¯¦ç»†çš„è¯æ±‡è¡¨æ ¼
- ğŸ¯ è€ƒç‚¹åˆ†æå’Œä¾‹å¥
- ğŸ“š æ–‡åŒ–èƒŒæ™¯çŸ¥è¯†
- ğŸ’¡ å­¦ä¹ å»ºè®®å’ŒæŠ€å·§

## ğŸ“ ä½¿ç”¨å»ºè®®
1. æŒ‰ç« èŠ‚é¡ºåºå­¦ä¹ ï¼Œæ¯å¤©1-2ç« 
2. é‡ç‚¹å…³æ³¨é«˜é¢‘è¯æ±‡å’Œè€ƒç‚¹åˆ†æ
3. ç»“åˆä¾‹å¥ç†è§£å•è¯ç”¨æ³•
4. å®šæœŸå¤ä¹ ï¼Œå·©å›ºè®°å¿†

*ğŸŒŸ ç¥æ‚¨è€ƒç ”è‹±è¯­å–å¾—ä¼˜å¼‚æˆç»©ï¼*
""")
    
    print(f"ğŸ“‹ ç”Ÿæˆäº†æ€»ç»“æŠ¥å‘Šï¼š{summary_file}")

if __name__ == "__main__":
    main()